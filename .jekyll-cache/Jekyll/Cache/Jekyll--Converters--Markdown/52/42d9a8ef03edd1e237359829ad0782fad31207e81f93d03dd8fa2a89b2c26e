I"¡<h2 id="introduction">Introduction</h2>
<p><strong>Proximal Gradient</strong>ç­‰ä»·äºimplicit gradient. ä»–çš„æ¢¯åº¦æ›´æ–°ä¸æ˜¯ç”¨\(x^k\)å¤„çš„gradient estimateï¼Œè€Œæ˜¯ç”¨åœ¨\(x^{k+1}\)çš„gradient estimate<br />
implicit gradient: \(x^{k+1} = x^{k} - \lambda \delta{f(x^{k+1})}\)<br />
explicit gradient descent: \(x^{k+1} = x^{k} - \lambda \delta{f(x^{k})}\)<br />
<strong>proximal update</strong>:  <br />
\(prox_f(x^k) = argmin f(x) + \frac{1}{2\lambda} \lVert x - x^k\rVert^2\)<br />
the optimal condition is: \(0 = \lambda \delta f(x^*) + (x^* - x^k)\)<br />
which is just the final update: \(x^* = x^k - \lambda \delta f(x^*)\).</p>

<p>Therefore, Proximal Gradient Algorithm is just a series of iteritive updating step to find the fixed point: \(x^* = prox_f(x^*)\)<br />
<strong>application</strong>ï¼š</p>

<h2 id="algorithm">Algorithm</h2>
<p>\(x^k = prox_{t_k, h} (y - t_k \delta g(y))\)<br />
\(y = x^k + \frac{t_k - 1}{t_{k + 1}} (x^k - x^{k-1})\)</p>

<h2 id="theory">Theory</h2>
<h3 id="proximal-operator">Proximal Operator</h3>
<p>if f is a convex function taking on finite values, then the proximal operator on f is :<br />
\(prox_f(v) = argmin_x(f(x) + \frac{1}{2} \lVert x-v \rVert^2_2)\)</p>
<ul>
  <li>the proximal operator of the scaled function \(\lambda f\), where \(\lambda &gt; 0\), which can be expressed as<br />
\(prox_{\lambda f}(\nu) = argmin_x (f(x) + 1/({2\lambda}) \lVert x - \nu \rVert_2^2\)<br />
the parameter \(\lambda\) control the extent to which the proximal operator maps points towards the minimum of \(f\), with larger values of \(\lambda\) associated with mapped points near the minimum, and smaller values giving a smaller movement towards the minimum.</li>
</ul>
<figure>
    <a href="https://thumbnail10.baidupcs.com/thumbnail/e154a9fcb76501d38c2c883fa7bb96a4?fid=1483288374-250528-257889517280387&amp;rt=pr&amp;sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-tDcsLsN5N14Rm2Ed0o93ymYgSlA%3d&amp;expires=8h&amp;chkbd=0&amp;chkv=0&amp;dp-logid=577735282787323296&amp;dp-callid=0&amp;time=1548421200&amp;size=c10000_u10000&amp;quality=90&amp;vuk=1483288374&amp;ft=image"><img src="https://thumbnail10.baidupcs.com/thumbnail/e154a9fcb76501d38c2c883fa7bb96a4?fid=1483288374-250528-257889517280387&amp;rt=pr&amp;sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-tDcsLsN5N14Rm2Ed0o93ymYgSlA%3d&amp;expires=8h&amp;chkbd=0&amp;chkv=0&amp;dp-logid=577735282787323296&amp;dp-callid=0&amp;time=1548421200&amp;size=c10000_u10000&amp;quality=90&amp;vuk=1483288374&amp;ft=image" /></a>
    <figcaption><a href="https://thumbnail10.baidupcs.com/thumbnail/e154a9fcb76501d38c2c883fa7bb96a4?fid=1483288374-250528-257889517280387&amp;rt=pr&amp;sign=FDTAER-DCb740ccc5511e5e8fedcff06b081203-tDcsLsN5N14Rm2Ed0o93ymYgSlA%3d&amp;expires=8h&amp;chkbd=0&amp;chkv=0&amp;dp-logid=577735282787323296&amp;dp-callid=0&amp;time=1548421200&amp;size=c10000_u10000&amp;quality=90&amp;vuk=1483288374&amp;ft=image" title="Proximal Gradient Method">Explain The Algorithm</a>.</figcaption>
</figure>

<p>ä¹Ÿå°±æ˜¯è¯´ï¼Œ å¦‚æœæ˜¯åœ¨å®šä¹‰åŸŸé‡Œé¢çš„ç‚¹ï¼Œ proximal operatoræ˜¯ä½¿è¯¥ç‚¹å‘æœ€å°åŒ–æ–¹å‘èµ°ï¼Œå¯¹äºä¸åœ¨å®šä¹‰åŸŸçš„ç‚¹ï¼Œ ä¼šä½¿è¯¥ç‚¹å‘æœ€å°åŒ–ä¸”æ˜¯åœ¨å®šä¹‰åŸŸå†…çš„æ–¹å‘èµ°ã€‚</p>

<h3 id="ç®—æ³•çš„æ”¶æ•›æ€§åˆ†æ">ç®—æ³•çš„æ”¶æ•›æ€§åˆ†æï¼š</h3>
<p>Proximal Operatorå…·æœ‰Nonexpansive Operatorçš„æ€§è´¨ï¼Œå³ï¼š<br />
\(\lVert prox_f(x) - prox_f(y) \rVert_2^2 \le (x - y)^T (prox_f(x) - prox_f(y))\); where \(x, y \in R^n\).<br />
å¯ä»¥è¯æ˜çš„æ˜¯å¦‚æœNæ˜¯nonexpansiveï¼Œ é‚£ä¹ˆoperatorï¼š\(T = (1-\alpha)I + \alpha N\), where \(\alpha \in (0, 1 )\)ï¼Œ has fixed point. å› ä¸ºNå’ŒTçš„è¿­ä»£ä¼šæ”¶æ•›åˆ°fixed point T. i.e. \(x^{k+1} = (1-\alpha) x^{k} + \alpha N(x^k)\) will converge to a fixed point of N.</p>

:ET